{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjwdWD7uqymZ"
      },
      "source": [
        "import package needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVihlGFzIK8S"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GzxwdD_qtUQ"
      },
      "source": [
        "download dataset(face celeba....) from website keggle\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je14DGrCdlEx"
      },
      "source": [
        "# !pip install kaggle \n",
        "# !pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json  # set permission\n",
        "!kaggle datasets download -d jessicali9530/celeba-dataset\n",
        "# !kaggle datasets download -d gasgallo/faces-data-new\n",
        "!unzip \"/content/celeba-dataset.zip\"\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-haoNIGarHVa"
      },
      "source": [
        "read dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S2quU9ugIch"
      },
      "source": [
        "import os\n",
        "import PIL\n",
        "import cv2\n",
        "import glob\n",
        "height=32\n",
        "BATCH_SIZE=32\n",
        "weight=32\n",
        "channle=3\n",
        "images=glob.glob(\"/content/img_align_celeba/img_align_celeba/*.jpg\")\n",
        "train_dataset=[]\n",
        "i=0\n",
        "for i in range(10000):\n",
        "  img=PIL.Image.open(images[i]).resize((height,weight),PIL.Image.ANTIALIAS)\n",
        "  train_dataset.append(np.array(img))\n",
        "\n",
        "train_dataset=np.reshape(train_dataset,(10000,height,weight,channle)).astype(\"float32\")\n",
        "train_dataset=train_dataset/127.5-1.0\n",
        "\n",
        "train_dataset=tf.data.Dataset.from_tensor_slices(train_dataset).shuffle(10000).batch(BATCH_SIZE)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA0uGbsL6TpT"
      },
      "source": [
        "def build_generator(seed_size,channles):\n",
        "  model=tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(4*4*256,input_shape=(seed_size,)))\n",
        "  model.add(tf.keras.layers.BatchNormalization(momentum=.8))\n",
        "  model.add(tf.keras.layers.LeakyReLU(.2))\n",
        "\n",
        "\n",
        "  model.add(tf.keras.layers.Reshape((4,4,256)))\n",
        "\n",
        "  # model.add(tf.keras.layers.UpSampling2D())\n",
        "  # model.add(tf.keras.layers.Conv2D(256,3,padding=\"same\"))\n",
        "  model.add(tf.keras.layers.Conv2DTranspose(256,3,strides=2,padding=\"same\"))\n",
        "  model.add(tf.keras.layers.BatchNormalization(momentum=.8))\n",
        "  model.add(tf.keras.layers.LeakyReLU(.2))\n",
        "\n",
        "\n",
        "  # model.add(tf.keras.layers.UpSampling2D())\n",
        "  # model.add(tf.keras.layers.Conv2D(256,3,padding=\"same\"))\n",
        "  model.add(tf.keras.layers.Conv2DTranspose(128,3,strides=2,padding=\"same\"))\n",
        "  model.add(tf.keras.layers.BatchNormalization(momentum=.8))\n",
        "  model.add(tf.keras.layers.LeakyReLU(.2))\n",
        "\n",
        "  # model.add(tf.keras.layers.UpSampling2D())\n",
        "  # model.add(tf.keras.layers.Conv2D(128,3,padding=\"same\"))\n",
        "  model.add(tf.keras.layers.Conv2DTranspose(64,3,strides=1,padding=\"same\"))\n",
        "  model.add(tf.keras.layers.BatchNormalization(momentum=.8))\n",
        "  model.add(tf.keras.layers.LeakyReLU(.2))\n",
        "\n",
        "  # model.add(tf.keras.layers.UpSampling2D((3,3)))\n",
        "  model.add(tf.keras.layers.Conv2D(128,3,padding=\"same\"))\n",
        "  model.add(tf.keras.layers.Conv2DTranspose(32,3,strides=1,padding=\"same\"))\n",
        "  model.add(tf.keras.layers.BatchNormalization(momentum=.8))\n",
        "  model.add(tf.keras.layers.LeakyReLU(.2))\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2DTranspose(16,3,strides=2,padding=\"same\"))\n",
        "  model.add(tf.keras.layers.BatchNormalization(momentum=.8))\n",
        "  model.add(tf.keras.layers.LeakyReLU(.2))\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(3,3,padding=\"same\",activation=\"tanh\"))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3xS7Mtq8uo5"
      },
      "source": [
        "def build_descreminator(image_shape):\n",
        "  model=tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Conv2D(32,3,strides=2,padding=\"same\",input_shape=image_shape))\n",
        "  model.add(tf.keras.layers.LeakyReLU(.2))\n",
        "\n",
        "  model.add(tf.keras.layers.Dropout(.5))\n",
        "  model.add(tf.keras.layers.Conv2D(64,3,strides=2,padding=\"same\"))\n",
        "  model.add(tf.keras.layers.BatchNormalization(momentum=.8))\n",
        "  model.add(tf.keras.layers.LeakyReLU(.2))\n",
        "\n",
        "  model.add(tf.keras.layers.Dropout(.5))\n",
        "  model.add(tf.keras.layers.Conv2D(256,3,strides=2,padding=\"same\"))\n",
        "  model.add(tf.keras.layers.BatchNormalization(momentum=.8))\n",
        "  model.add(tf.keras.layers.LeakyReLU(.2))\n",
        "\n",
        "\n",
        "  model.add(tf.keras.layers.Dropout(.5))\n",
        "  model.add(tf.keras.layers.Conv2D(512,3,strides=2,padding=\"same\"))\n",
        "  model.add(tf.keras.layers.BatchNormalization(momentum=.8))\n",
        "  model.add(tf.keras.layers.LeakyReLU(.2))\n",
        "\n",
        "\n",
        "  model.add(tf.keras.layers.Dropout(.5))\n",
        "  model.add(tf.keras.layers.Conv2D(1024,3,strides=2,padding=\"same\"))\n",
        "  model.add(tf.keras.layers.BatchNormalization(momentum=.8))\n",
        "  model.add(tf.keras.layers.LeakyReLU(.2))\n",
        "\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(1,activation=\"sigmoid\"))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogT7z9npj44z"
      },
      "source": [
        "cross_ent=tf.keras.losses.BinaryCrossentropy(from_logits=1)\n",
        "def discreminator_loss(real,fake):\n",
        "  real_loss=cross_ent(tf.ones_like(real),real)\n",
        "  fake_loss=cross_ent(tf.zeros_like(fake),fake)\n",
        "  return real_loss+fake_loss\n",
        "\n",
        "def generate_loss(fake):\n",
        "  return cross_ent(tf.ones_like(fake),fake)\n",
        "\n",
        "generate_optimizer=tf.keras.optimizers.Adam(1.5e-4,.5)\n",
        "discreminator_optimizer=tf.keras.optimizers.Adam(1.5e-4,.5)\n",
        "generator=build_generator(seed_size=100,channles=3)\n",
        "discreminator=build_descreminator((height,weight,channle))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfuDr9uH5fK7"
      },
      "source": [
        "@tf.function\n",
        "def train_step(images):\n",
        "  seed=tf.random.normal([BATCH_SIZE,100])\n",
        "  with tf.GradientTape() as gene_tape,tf.GradientTape() as discr_tape:\n",
        "    generate_image=generator(seed,training=1)\n",
        "\n",
        "\n",
        "    real=discreminator(images,training=1)\n",
        "    fake=discreminator(generate_image,training=1)\n",
        "\n",
        "    gene_loss=generate_loss(fake)\n",
        "    discr_loss=discreminator_loss(real,fake)\n",
        "\n",
        "    gradient_gene=gene_tape.gradient(gene_loss,generator.trainable_variables)\n",
        "    gradient_disct=discr_tape.gradient(discr_loss,discreminator.trainable_variables)\n",
        "\n",
        "    generate_optimizer.apply_gradients(zip(gradient_gene,generator.trainable_variables))\n",
        "    discreminator_optimizer.apply_gradients(zip(gradient_disct,discreminator.trainable_variables))\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qcKepvnAZvV"
      },
      "source": [
        "PREVIEW_MARGIN=16\n",
        "PREVIEW_ROWS=1\n",
        "PREVIEW_COLS=1\n",
        "GENERATE_SQUARE=32\n",
        "SEED_SIZE=100\n",
        "def save_images(cnt,noise):\n",
        "  image_array = np.full(( \n",
        "      PREVIEW_MARGIN + (PREVIEW_ROWS * (GENERATE_SQUARE+PREVIEW_MARGIN)), \n",
        "      PREVIEW_MARGIN + (PREVIEW_COLS * (GENERATE_SQUARE+PREVIEW_MARGIN)), 3), \n",
        "      255, dtype=np.uint8)\n",
        "  \n",
        "  generated_images = generator.predict(noise)\n",
        "\n",
        "  generated_images = 0.5 * generated_images + 0.5\n",
        "\n",
        "  image_count = 0\n",
        "  for row in range(PREVIEW_ROWS):\n",
        "      for col in range(PREVIEW_COLS):\n",
        "        r = row * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n",
        "        c = col * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n",
        "        image_array[r:r+GENERATE_SQUARE,c:c+GENERATE_SQUARE] \\\n",
        "            = generated_images[image_count] * 255\n",
        "        image_count += 1\n",
        "\n",
        "          \n",
        "  output_path = os.path.join(\"/content\",'output')\n",
        "  if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)\n",
        "  \n",
        "  filename = os.path.join(output_path,f\"train-{cnt}.png\")\n",
        "  im =PIL.Image.fromarray(image_array)\n",
        "  im.save(filename)\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klRx1EimUyzw"
      },
      "source": [
        "def train(dataset,epochs):\n",
        "  fixed_seed = np.random.normal(0, 1, (PREVIEW_ROWS * PREVIEW_COLS, \n",
        "                                       SEED_SIZE))\n",
        "  for epoch in range(epochs):\n",
        "    gen_loss=[]\n",
        "    dis_loss=[]\n",
        "    for batch_image in dataset:\n",
        "      train_step(batch_image)\n",
        "    \n",
        "    if epoch%2!=0:\n",
        "      generator.save_weights(\"weights_generator_GAN.h5\")\n",
        "      discreminator.save_weights(\"weights_discreminator_GAN.h5\")\n",
        "      !cp weights_discreminator_GAN.h5 -r \"/content/drive/My Drive\"\n",
        "      !cp weights_generator_GAN.h5 -r \"/content/drive/My Drive\"\n",
        "      \n",
        "    save_images(epoch,fixed_seed)\n",
        "    print(epoch)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yss7uUaVtq-"
      },
      "source": [
        "train(train_dataset,200)"
      ],
      "execution_count": 33,
      "outputs": []
    }
  ]
}